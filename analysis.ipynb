{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# see full details @ https://gracejiang.github.io/cis545/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import json\n",
        "from pandas import json_normalize\n",
        "import requests\n",
        "import glob\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import os\n",
        "import fnmatch\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import sklearn\n",
        "import scipy.stats as st\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "# loading in all msgs into the dataframe all_msgs\n",
        "#\n",
        "#\n",
        "files_path = 'all-messages/inbox/'\n",
        "\n",
        "all_msgs = pd.DataFrame()\n",
        "\n",
        "for root, dir, files in os.walk(files_path):\n",
        "    for json_file in fnmatch.filter(files, \"*.json\"):\n",
        "        file_url = root + '/' + json_file\n",
        "        if not ('file' in file_url):\n",
        "            with open(file_url) as json_data:\n",
        "                data = json.load(json_data)\n",
        "            print(file_url)\n",
        "            curr_json_df = json_normalize(data, 'messages')\n",
        "            all_msgs = pd.concat([all_msgs, curr_json_df])\n",
        "            \n",
        "\n",
        "all_msgs.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "# make a copy of all_msgs, since i don't want to have to rerun previous step (takes a long time) in the case that i accidentally override data\n",
        "#\n",
        "# comment out lines when appropriate\n",
        "#\n",
        "#\n",
        "\n",
        "# all_msgs_copy = all_msgs.copy()\n",
        "# all_msgs = all_msgs_copy.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# total number of convos\n",
        "files_path = 'all-messages/inbox/'\n",
        "\n",
        "all_roots = set()\n",
        "for root, dir, files in os.walk(files_path):\n",
        "    for json_file in fnmatch.filter(files, \"*.json\"):\n",
        "        file_url = root + '/' + json_file\n",
        "        if not ('file' in file_url): \n",
        "            all_roots.add(file_url)\n",
        "            \n",
        "len(all_roots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "# convert columns into correct data type\n",
        "#\n",
        "#\n",
        "\n",
        "# drop nan values\n",
        "all_msgs = all_msgs.dropna(subset=['content'])\n",
        "\n",
        "# convert timestamp to datetime format\n",
        "all_msgs['datetime'] = all_msgs.apply(lambda row: datetime.datetime.fromtimestamp(int(row.timestamp_ms) * 0.001), axis = 1)\n",
        "\n",
        "# separate date and time from datetime\n",
        "all_msgs['date'] = [d.date() for d in all_msgs['datetime']]\n",
        "all_msgs['month'] = [d.month for d in all_msgs['datetime']]\n",
        "all_msgs['year'] = [d.year for d in all_msgs['datetime']]\n",
        "all_msgs['time'] = [d.time() for d in all_msgs['datetime']]\n",
        "\n",
        "# select only certain columns\n",
        "all_msgs = all_msgs[['sender_name', 'date', 'month', 'year', 'time', 'content', 'reactions', 'datetime']]\n",
        "\n",
        "# rename column sender_name to name\n",
        "all_msgs = all_msgs.rename(columns={'sender_name': 'name'})\n",
        "\n",
        "# sort by datetime\n",
        "all_msgs = all_msgs.sort_values(by=['datetime'])\n",
        "\n",
        "# only msgs from 2012 and later\n",
        "all_msgs = all_msgs[all_msgs['year'] >= 2012]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_msgs.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# total number of msgs exchagned on facebook\n",
        "print(len(all_msgs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataframe to count messages from all people except me\n",
        "\n",
        "other_people_df = all_msgs[all_msgs['name'] != 'Grace Jiang']\n",
        "my_msgs_df = all_msgs[all_msgs['name'] == 'Grace Jiang']\n",
        "\n",
        "# how many msgs other people sent me\n",
        "print(len(other_people_df))\n",
        "\n",
        "# how many msgs i sent to other people\n",
        "print(len(my_msgs_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot of total number of messages sent per day over time\n",
        "msgs_per_date = all_msgs.groupby('date')\n",
        "counts = msgs_per_date.date.count()\n",
        "counts.plot(kind=\"line\")\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ok, so this graph, is too jumpy, so i'm going to break down by per year to smooth it out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot total number of messages sent & received per year over time\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# total number of messages sent & received\n",
        "plot_all_msgs_df = all_msgs.groupby(['year'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "# number of msgs i sent\n",
        "plot_sent_msgs_df = my_msgs_df.groupby(['year'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "# number of msgs i received\n",
        "plot_received_msgs_df = other_people_df.groupby(['year'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "ax.plot(plot_all_msgs_df['year'], plot_all_msgs_df['content'], label='total messages')\n",
        "ax.plot(plot_sent_msgs_df['year'], plot_sent_msgs_df['content'], label='messages sent')\n",
        "ax.plot(plot_received_msgs_df['year'], plot_received_msgs_df['content'], label='messages received')\n",
        "\n",
        "plt.title(\"messages sent & received over time\", loc='center', fontsize=14, fontweight=0, color='black')\n",
        "ax.set_xlabel(\"year\")\n",
        "ax.set_ylabel(\"number of messages\")\n",
        "ax.legend(loc='best')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "#\n",
        "# count number of messages received per person\n",
        "#\n",
        "#\n",
        "\n",
        "# msgs_per_person = all_msgs.groupby(['name']).count() \n",
        "# msgs_per_person = msgs_per_person['content']\n",
        "\n",
        "msgs_per_person = other_people_df['name'].value_counts()\n",
        "msgs_per_person.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count messages per person per month & year\n",
        "# only include people with at least 25k messages received (~50k total messages, indicating significant talking)\n",
        "\n",
        "close_friends_series = msgs_per_person[msgs_per_person >= 25000]\n",
        "\n",
        "close_friends = set(close_friends_series.index)\n",
        "for friend in close_friends:\n",
        "    print(friend)\n",
        "\n",
        "#\n",
        "# filter messages to include only close friends\n",
        "#\n",
        "\n",
        "close_friends_df = other_people_df[other_people_df['name'].isin(close_friends)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot number of messages exchanged with my close friends over time\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "plot_cf_df = close_friends_df.groupby(['year', 'name'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "for name in close_friends:\n",
        "    ax.plot(plot_cf_df[plot_cf_df.name == name].year, plot_cf_df[plot_cf_df.name == name].content,label=name)\n",
        "\n",
        "plt.title(\"messages received from close friends over time\", loc='center', fontsize=14, fontweight=0, color='black')\n",
        "ax.set_xlabel(\"year\")\n",
        "ax.set_ylabel(\"number of messages\")\n",
        "ax.legend(loc='best')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# graph excluding outlier\n",
        "outlier_name = 'Name Here' #edit this to contain your own outlier\n",
        "\n",
        "excluding_outliers = close_friends.copy()\n",
        "excluding_outliers.remove(outlier_name)\n",
        "\n",
        "excluding_outliers_df = other_people_df[other_people_df['name'].isin(excluding_outliers)]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "plot_cf_df = excluding_outliers_df.groupby(['year', 'name'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "for name in excluding_outliers:\n",
        "    ax.plot(plot_cf_df[plot_cf_df.name == name].year, plot_cf_df[plot_cf_df.name == name].content,label=name)\n",
        "\n",
        "plt.title(\"messages received from close friends excluding outliers\", loc='center', fontsize=14, fontweight=0, color='black')\n",
        "ax.set_xlabel(\"year\")\n",
        "ax.set_ylabel(\"number of messages\")\n",
        "ax.legend(loc='best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyzing how my language changes over time\n",
        "# word clouds!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stopwords\n",
        "# most commonly used words\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.extend(['ur', 'u', 'like', 'ok', 'im', 'yea', 'Ã¢', 'dont', 'oh', 'yeah', 'idk', 'also', 'thats', 'i', 'and', 'the', 'a', 'but', 'so', 'then', 'bc', 'cuz'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# splitting into wordclouds over years\n",
        "\n",
        "my_msgs_2012 = my_msgs_df[my_msgs_df['year'] == 2012]\n",
        "my_msgs_2013 = my_msgs_df[my_msgs_df['year'] == 2013]\n",
        "my_msgs_2014 = my_msgs_df[my_msgs_df['year'] == 2014]\n",
        "my_msgs_2015 = my_msgs_df[my_msgs_df['year'] == 2015]\n",
        "my_msgs_2016 = my_msgs_df[my_msgs_df['year'] == 2016]\n",
        "my_msgs_2017 = my_msgs_df[my_msgs_df['year'] == 2017]\n",
        "my_msgs_2018 = my_msgs_df[my_msgs_df['year'] == 2018]\n",
        "my_msgs_2019 = my_msgs_df[my_msgs_df['year'] == 2019]\n",
        "my_msgs_2020 = my_msgs_df[my_msgs_df['year'] == 2020]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "outputs": [],
      "source": [
        "# generate all words in a list\n",
        "\n",
        "def generate_words_list(df):\n",
        "    # split content into lists of words\n",
        "    split_words = df.content.str.lower().str.split()\n",
        "    split_words_df = pd.DataFrame(split_words)\n",
        "\n",
        "    # iterate through each word and add word to all_words_list\n",
        "    # count_var = 0\n",
        "    all_words_list = list()\n",
        "    for index, row in split_words_df.iterrows():\n",
        "        if (type(row.content) == list):\n",
        "            for word in row.content:\n",
        "                if word not in stopwords:\n",
        "                    all_words_list.append(word)\n",
        "            # print(str(count_var), \": \", row.content)\n",
        "            # count_var = count_var + 1\n",
        "\n",
        "    # convert all_words_list to dataframe\n",
        "    all_words_df = pd.DataFrame(all_words_list, columns=['word'])\n",
        "    all_words_df.head()\n",
        "\n",
        "    #\n",
        "    # uncomment the following code below to see the count of\n",
        "    # number of words (see most popular words per year)\n",
        "    #\n",
        "    # counts = all_words_df.groupby('word')\\\n",
        "    #     .word.value_counts()\\\n",
        "    #     .to_frame()\\\n",
        "    #     .rename(columns={'word':'count'})\n",
        "\n",
        "    # counts = counts.sort_values(by=['count'], ascending=False)\n",
        "    # counts.head(15)\n",
        "\n",
        "    return all_words_list\n",
        "\n",
        "# generate wordcloud\n",
        "def generate_wordcloud(df, title):\n",
        "    all_words_list = generate_words_list(df)\n",
        "    wordcloud = WordCloud(\n",
        "        width = 1500,\n",
        "        height = 1000,\n",
        "        background_color = 'black',\n",
        "        stopwords = STOPWORDS).generate(str(all_words_list))\n",
        "    fig = plt.figure(\n",
        "        figsize = (40, 30),\n",
        "        facecolor = 'k',\n",
        "        edgecolor = 'k')\n",
        "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.title(title, loc='center', fontsize=80, color='white')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2020, \"Most Common Words I Used in 2020\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2019, \"Most Common Words I Used in 2019\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2018, \"Most Common Words I Used in 2018\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2017, \"Most Common Words I Used in 2017\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2016, \"Most Common Words I Used in 2016\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2015, \"Most Common Words I Used in 2015\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2014, \"Most Common Words I Used in 2014\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2013, \"Most Common Words I Used in 2013\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_wordcloud(my_msgs_2012, \"Most Common Words I Used in 2012\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sentiment analysis for all my messages over the years\n",
        "from textblob import TextBlob\n",
        "\n",
        "def find_sentiment_analysis(df):\n",
        "    sentiment = 0.0\n",
        "    num_msgs = 0.0\n",
        "    for row in df.content.str.lower():\n",
        "        blob = TextBlob(row)\n",
        "        sentiment += blob.sentiment.polarity\n",
        "        num_msgs += 1\n",
        "    return sentiment / num_msgs * 100.0\n",
        "\n",
        "sentiment_analysis = []\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2012))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2013))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2014))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2015))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2016))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2017))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2018))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2019))\n",
        "sentiment_analysis.append(find_sentiment_analysis(my_msgs_2020))\n",
        "\n",
        "my_sentiments_df = pd.DataFrame({'sentiment': sentiment_analysis}, index=[2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020])\n",
        "my_sentiments_df.plot.line()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# finding what kind of regression to use to predict my future 2020 messaging trends with my friend\n",
        "\n",
        "path_to_user = 'messages/inbox/acbubba_wggssp40wq/'\n",
        "json_files = os.path.join(path_to_user, '*.json')\n",
        "files = glob.glob(json_files)\n",
        "\n",
        "ac_df = pd.DataFrame()\n",
        "\n",
        "# read in all jsons in a persons's message folder\n",
        "for file_url in files:\n",
        "    with open(file_url) as json_data:\n",
        "        data = json.load(json_data)\n",
        "    curr_json_df = json_normalize(data, 'messages')\n",
        "    ac_df = ac_df.append(curr_json_df, ignore_index = True)\n",
        "\n",
        "ac_df.head(15)\n",
        "\n",
        "# drop nans\n",
        "ac_df = ac_df.dropna(subset=['content'])\n",
        "\n",
        "# convert timestamp to datetime format\n",
        "ac_df['datetime'] = ac_df.apply(lambda row: datetime.datetime.fromtimestamp(int(row.timestamp_ms) * 0.001), axis = 1) \n",
        "\n",
        "# separate date and time from datetime\n",
        "ac_df['date'] = [d.date() for d in ac_df['datetime']]\n",
        "ac_df['month'] = [d.month for d in ac_df['datetime']]\n",
        "ac_df['year'] = [d.year for d in ac_df['datetime']]\n",
        "ac_df['time'] = [d.time() for d in ac_df['datetime']]\n",
        "\n",
        "# select only certain columns\n",
        "ac_df = ac_df[['sender_name', 'date', 'month', 'year', 'time', 'content', 'reactions', 'datetime']]\n",
        "\n",
        "# proper year month\n",
        "def convert_month_str(month):\n",
        "    if (month < 10):\n",
        "        return '0' + str(month)\n",
        "    return str(month)\n",
        "ac_df['month'] = ac_df['month'].apply(lambda x: convert_month_str(x))\n",
        "ac_df['year-month'] = ac_df['year'].astype(str) + '-' + ac_df['month'].astype(str)\n",
        "\n",
        "# rename column sender_name to name\n",
        "ac_df = ac_df.rename(columns={'sender_name': 'name'})\n",
        "\n",
        "# sort by datetime\n",
        "ac_df = ac_df.sort_values(by=['datetime'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ac_df.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot number of messages sent & received to/from ac over time\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# total number of messages sent & received between us\n",
        "plot_ac_df = ac_df.groupby(['year-month'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "# i sent to ac / ac sent to me:\n",
        "ac_to_me_df = ac_df[ac_df['name'] != 'Grace Jiang']\n",
        "me_to_ac_df = ac_df[ac_df['name'] == 'Grace Jiang']\n",
        "\n",
        "# number of msgs i sent\n",
        "plot_sent_msgs_df = me_to_ac_df.groupby(['year-month'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "# number of msgs i received\n",
        "plot_received_msgs_df = ac_to_me_df.groupby(['year-month'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "ax.plot(plot_ac_df['year-month'], plot_ac_df['content'], label='total messages exchanged')\n",
        "ax.plot(plot_sent_msgs_df['year-month'], plot_sent_msgs_df['content'], label='messages i sent')\n",
        "ax.plot(plot_received_msgs_df['year-month'], plot_received_msgs_df['content'], label='messages i received')\n",
        "\n",
        "# plotting\n",
        "\n",
        "ax.plot(plot_ac_df['year-month'], plot_ac_df['content'])\n",
        "\n",
        "plt.title(\"messages sent & received to/from ac over time\", loc='center', fontsize=14, fontweight=0, color='black')\n",
        "ax.set_xlabel(\"year and month\")\n",
        "ax.set_ylabel(\"number of messages\")\n",
        "plt.xticks(rotation=90)\n",
        "ax.legend(loc='best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nlp sentiment analysis for each person (how positive they are)\n",
        "\n",
        "ac_phrases = []\n",
        "ac_sentiment = 0.0\n",
        "ac_msgs = 0.0\n",
        "for row in ac_to_me_df.content.str.lower():\n",
        "    ac_phrases.append(row)\n",
        "    blob = TextBlob(row)\n",
        "    ac_sentiment += blob.sentiment.polarity\n",
        "    ac_msgs += 1\n",
        "\n",
        "print(\"AC Sentiment Analysis Ratio: \")\n",
        "print(ac_sentiment / ac_msgs * 100.0)\n",
        "\n",
        "grac_phrases = []\n",
        "grac_sentiment = 0.0\n",
        "grac_msgs = 0.0\n",
        "for row in me_to_ac_df.content.str.lower():\n",
        "    grac_phrases.append(row)\n",
        "    blob = TextBlob(row)\n",
        "    grac_sentiment += blob.sentiment.polarity\n",
        "    grac_msgs += 1\n",
        "\n",
        "print(\"Grac Sentiment Analysis Ratio: \")\n",
        "print(grac_sentiment / grac_msgs * 100.0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# linear regression: simple\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "new_lr_ac_df = ac_df.groupby(['year-month', 'name'], as_index=False).agg({'content': 'count'})\n",
        "\n",
        "def year_in_num(year_month):\n",
        "    year = int(year_month[:4])\n",
        "    month = int(year_month[5:])\n",
        "    raw_value = year * 12 + month\n",
        "    return raw_value - (2019 * 12 + 7)\n",
        "\n",
        "ac_lr_df = new_lr_ac_df[new_lr_ac_df['name'] == 'AC Bubba']\n",
        "ac_lr_df['time'] = ac_lr_df['year-month'].apply(lambda x: year_in_num(x))\n",
        "ac_lr_df = ac_lr_df.dropna(subset=['year-month'])\n",
        "\n",
        "grac_lr_df = new_lr_ac_df[new_lr_ac_df['name'] == 'Grace Jiang']\n",
        "grac_lr_df['time'] = grac_lr_df['year-month'].apply(lambda x: year_in_num(x))\n",
        "grac_lr_df = grac_lr_df.dropna(subset=['year-month'])\n",
        "\n",
        "ac_X = ac_lr_df['time'].values.reshape(-1, 1)\n",
        "ac_Y = ac_lr_df['content'].values.reshape(-1, 1)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(ac_X, ac_Y)\n",
        "ac_Y_pred = lr.predict(ac_X)\n",
        "\n",
        "plt.scatter(ac_X, ac_Y)\n",
        "plt.title(\"AC's predicted monthly messages\", loc='center', fontsize=14, fontweight=0, color='black')\n",
        "plt.plot(ac_X, ac_Y_pred, color='red')\n",
        "plt.show()\n",
        "\n",
        "grac_X = grac_lr_df['time'].values.reshape(-1, 1)\n",
        "grac_Y = grac_lr_df['content'].values.reshape(-1, 1)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(grac_X, grac_Y)\n",
        "grac_Y_pred = lr.predict(grac_X)\n",
        "\n",
        "plt.scatter(grac_X, grac_Y)\n",
        "plt.title(\"Grace's predicted monthly messages\", loc='center', fontsize=14, fontweight=0, color='black')\n",
        "plt.plot(grac_X, grac_Y_pred, color='blue')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# linear regression: more modelling\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import sklearn\n",
        "from matplotlib import pyplot as plt \n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr_ac_df = ac_df.groupby(['year-month', 'name'], as_index=False).agg({'content': 'count'})\n",
        "lr_ac_df['name'] = lr_ac_df['name'].astype('category')\n",
        "lr_ac_df['year'] = lr_ac_df['year-month'].apply(lambda x: x[:4]).astype(int)\n",
        "lr_ac_df['month'] = lr_ac_df['year-month'].apply(lambda x: x[5:]).astype(int)\n",
        "\n",
        "lr_ac_df = lr_ac_df.drop(columns=['year-month'])\n",
        "\n",
        "lr_ac_df = pd.get_dummies(lr_ac_df, columns=['name'])\n",
        "\n",
        "label = lr_ac_df['content']\n",
        "features = lr_ac_df.drop(columns=['content'])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, label, test_size=0.2)\n",
        "\n",
        "lin_regressor = LinearRegression()\n",
        "lin_regressor.fit(x_train, y_train)\n",
        "y_pred = lin_regressor.predict(x_test)\n",
        "\n",
        "mse_test = mean_squared_error(y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dimensionality reduction with PCA \n",
        "x_df = pd.DataFrame(x_train)\n",
        "\n",
        "pca = PCA()\n",
        "to_train_pca = sklearn.preprocessing.StandardScaler().fit_transform(x_df)\n",
        "trained_pca = pca.fit_transform(to_train_pca)\n",
        "\n",
        "plt.plot(trained_pca)\n",
        "plt.show()\n",
        "\n",
        "evr = pca.explained_variance_ratio_\n",
        "components = pca.components_\n",
        "ratio_plot = np.cumsum(evr)\n",
        "plt.plot(ratio_plot)\n",
        "\n",
        "new_pca = PCA(n_components=4)\n",
        "x_train = new_pca.fit_transform(x_train)\n",
        "\n",
        "rfr = RandomForestRegressor(random_state=4)\n",
        "parameters = {\n",
        "    'max_depth': [2, 4], 'n_estimators': [1, 2, 3]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rfr, param_grid=parameters)\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "x_test = new_pca.fit_transform(x_test)\n",
        "y_pred = grid_search.best_estimator_.predict(x_test)\n",
        "\n",
        "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# machine learning to categorize who sent the message based off language analysis\n",
        "\n",
        "\n",
        "data = []\n",
        "data_labels = []\n",
        "for row in ac_to_me_df.content.str.lower():\n",
        "    data.append(row)\n",
        "    data_labels.append('ac')\n",
        "\n",
        "for row in me_to_ac_df.content.str.lower():\n",
        "    data.append(row)\n",
        "    data_labels.append('grac')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# machine learning to categorize who sent the message based off language analysis\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    analyzer = 'word',\n",
        "    lowercase = False,\n",
        ")\n",
        "\n",
        "features = vectorizer.fit_transform(data)\n",
        "features_nd = features.toarray() # for easy usage\n",
        "\n",
        "X_train, X_test, y_train, y_test  = train_test_split(\n",
        "        features_nd, \n",
        "        data_labels,\n",
        "        train_size=0.80, \n",
        "        random_state=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# building linear classifier\n",
        "\n",
        "log_model = LogisticRegression()\n",
        "log_model = log_model.fit(X=X_train, y=y_train)\n",
        "y_pred = log_model.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "proj.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python37364bit373pyenve1bdd8fc0ad645b28d3c8387089ae4c8",
      "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}